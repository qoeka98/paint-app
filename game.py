import streamlit as st
import tensorflow as tf
import numpy as np
import os
import cv2
import time
from PIL import Image
from streamlit_webrtc import webrtc_streamer, VideoTransformerBase
import av

# âœ… ëª¨ë¸ ë¡œë“œ
model_path = "model/keras_model.h5"
if not os.path.exists(model_path):
    st.error(f"âŒ ëª¨ë¸ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {model_path}")
    st.stop()

model = tf.keras.models.load_model(model_path)
class_names = ["ê°€ìœ„", "ë°”ìœ„", "ë³´"]

# âœ… ì¹´ë©”ë¼ ì„ íƒ ê¸°ëŠ¥ ì¶”ê°€ (USB ì›¹ìº  ì§€ì›)
st.sidebar.title("ğŸ“· ì¹´ë©”ë¼ ì„¤ì •")
camera_option = st.sidebar.radio("ì¹´ë©”ë¼ ì„ íƒ", ["ğŸ”— ë‚´ì¥ ì›¹ìº  (WebRTC)", "ğŸ”Œ USB ì›¹ìº  (OpenCV)"])

# âœ… ì„¸ì…˜ ë³€ìˆ˜ ì´ˆê¸°í™”
if "monster_mp" not in st.session_state:
    st.session_state.monster_mp = 50  
if "initial_mp" not in st.session_state:
    st.session_state.initial_mp = st.session_state.monster_mp
if "game_running" not in st.session_state:
    st.session_state.game_running = True

st.subheader("ğŸ® ê°€ìœ„ë°”ìœ„ë³´ ëª¬ìŠ¤í„° ë°°í‹€ ê²Œì„")
st.info("ğŸ“¸ ì›¹ìº ì„ ì¼œê³  ì´ˆë¡ìƒ‰ ë„¤ëª¨ ì•ˆì— ì†ì„ ìœ„ì¹˜ì‹œí‚¤ì„¸ìš”!")

# âœ… 1. ğŸ”— **WebRTC ê¸°ë°˜ ìŠ¤íŠ¸ë¦¬ë° (ê¸°ë³¸ ì›¹ìº  ì‚¬ìš©)**
if camera_option == "ğŸ”— ë‚´ì¥ ì›¹ìº  (WebRTC)":
    class VideoTransformer(VideoTransformerBase):
        def __init__(self):
            self.last_captured_time = time.time()
            self.capture_interval = 3  # 3ì´ˆë§ˆë‹¤ ìº¡ì²˜
            self.frame_to_analyze = None

        def transform(self, frame):
            img = frame.to_ndarray(format="bgr24")
            h, w, _ = img.shape

            # âœ… ë„¤ëª¨ ë°•ìŠ¤ ì„¤ì • (í™”ë©´ ì¤‘ì•™)
            box_size = min(h, w) // 2
            x1, y1 = (w - box_size) // 2, (h - box_size) // 2
            x2, y2 = x1 + box_size, y1 + box_size
            cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)  # ì´ˆë¡ ë„¤ëª¨

            # âœ… 3ì´ˆë§ˆë‹¤ ì´ë¯¸ì§€ ìº¡ì²˜
            if time.time() - self.last_captured_time > self.capture_interval:
                self.frame_to_analyze = img[y1:y2, x1:x2]  # ë„¤ëª¨ ë°•ìŠ¤ ë‚´ë¶€ë§Œ ì €ì¥
                self.last_captured_time = time.time()

            return av.VideoFrame.from_ndarray(img, format="bgr24")

    webrtc_ctx = webrtc_streamer(
        key="game",
        video_transformer_factory=VideoTransformer,
        async_transform=True
    )

    # âœ… WebRTCì—ì„œ 3ì´ˆë§ˆë‹¤ ìº¡ì²˜ëœ í”„ë ˆì„ì„ ì‚¬ìš©í•˜ì—¬ ì˜ˆì¸¡
    if webrtc_ctx.video_transformer and webrtc_ctx.video_transformer.frame_to_analyze is not None:
        captured_img = webrtc_ctx.video_transformer.frame_to_analyze

        # âœ… ì´ë¯¸ì§€ ì „ì²˜ë¦¬
        img = cv2.cvtColor(captured_img, cv2.COLOR_BGR2RGB)
        img = Image.fromarray(img)
        img = img.resize((224, 224))
        img_array = np.array(img, dtype=np.float32) / 255.0
        img_array = np.expand_dims(img_array, axis=0)

# âœ… 2. ğŸ”Œ **OpenCVë¥¼ ì´ìš©í•œ USB ì›¹ìº  ì—°ê²°**
elif camera_option == "ğŸ”Œ USB ì›¹ìº  (OpenCV)":
    usb_camera_index = st.sidebar.number_input("ğŸ“· USB ì›¹ìº  ì¸ë±ìŠ¤ ì„¤ì • (ê¸°ë³¸ê°’: 0)", min_value=0, step=1, value=0)

    cap = cv2.VideoCapture(usb_camera_index)  # USB ì›¹ìº  ì—°ê²°

    if not cap.isOpened():
        st.error("âŒ USB ì›¹ìº ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì˜¬ë°”ë¥¸ ì¸ë±ìŠ¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”!")
        st.stop()

    stframe = st.empty()

    # âœ… ì‹¤ì‹œê°„ ì›¹ìº  í‘œì‹œ & ë„¤ëª¨ ë°•ìŠ¤
    while True:
        ret, frame = cap.read()
        if not ret:
            st.error("âŒ USB ì›¹ìº ì—ì„œ ì˜ìƒì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            break

        # âœ… ë„¤ëª¨ ë°•ìŠ¤ ì¶”ê°€
        h, w, _ = frame.shape
        box_size = min(h, w) // 2
        x1, y1 = (w - box_size) // 2, (h - box_size) // 2
        x2, y2 = x1 + box_size, y1 + box_size
        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)

        stframe.image(frame, channels="BGR")

        # âœ… 3ì´ˆ í›„ ì´¬ì˜
        time.sleep(3)
        roi = frame[y1:y2, x1:x2]

        # âœ… ì´ë¯¸ì§€ ì „ì²˜ë¦¬
        img = cv2.cvtColor(roi, cv2.COLOR_BGR2RGB)
        img = Image.fromarray(img)
        img = img.resize((224, 224))
        img_array = np.array(img, dtype=np.float32) / 255.0
        img_array = np.expand_dims(img_array, axis=0)

        break  # 3ì´ˆ í›„ 1íšŒë§Œ ì‹¤í–‰

    cap.release()

# âœ… 3. ğŸ¤– **AI ëª¨ë¸ ì˜ˆì¸¡**
prediction = model.predict(img_array)
class_index = np.argmax(prediction)
confidence = np.max(prediction)

if confidence >= 0.7:
    user_choice = class_names[class_index]
    monster_choice = np.random.choice(["ê°€ìœ„", "ë°”ìœ„", "ë³´"])

    game_result = "âš–ï¸ ë¹„ê¹€"
    if (user_choice == "ê°€ìœ„" and monster_choice == "ë³´") or \
       (user_choice == "ë°”ìœ„" and monster_choice == "ê°€ìœ„") or \
       (user_choice == "ë³´" and monster_choice == "ë°”ìœ„"):
        game_result = "âœ… ìŠ¹ë¦¬"
        st.session_state.monster_mp -= 10  
    elif user_choice != monster_choice:
        game_result = "âŒ íŒ¨ë°°"

    # âœ… ê²°ê³¼ ì¶œë ¥
    st.subheader(f"ğŸ– ë‚´ ì„ íƒ: {user_choice}  VS  ğŸ‘¾ ëª¬ìŠ¤í„° ì„ íƒ: {monster_choice}")
    st.markdown(f"### ê²°ê³¼ â¡ï¸ **{game_result}**")

    # âœ… MP ì§„í–‰ë¥  ë°” ì—…ë°ì´íŠ¸
    progress_value = max(st.session_state.monster_mp / st.session_state.initial_mp, 0)
    st.progress(progress_value)

    # âœ… ëª¬ìŠ¤í„° MPê°€ 0ì´ë©´ ê²Œì„ ì¢…ë£Œ
    if st.session_state.monster_mp <= 0:
        st.success("ğŸ‰ ëª¬ìŠ¤í„°ë¥¼ ë¬¼ë¦¬ì³¤ìŠµë‹ˆë‹¤! ê²Œì„ ì¢…ë£Œ!")
        st.session_state.game_running = False
