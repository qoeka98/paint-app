import tensorflow as tf
import numpy as np
import cv2
import time
import pandas as pd
import os
import streamlit as st
from PIL import Image
from streamlit_webrtc import webrtc_streamer, VideoTransformerBase
import av

# âœ… Teachable Machine ëª¨ë¸ ë¡œë“œ
model_path = "model/keras_model.h5"
if not os.path.exists(model_path):
    st.error(f"âŒ ëª¨ë¸ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {model_path}")
else:
    model = tf.keras.models.load_model(model_path)

# âœ… í´ë˜ìŠ¤ ë§¤í•‘
class_names = ["ê°€ìœ„", "ë°”ìœ„", "ë³´"]

# âœ… CSV íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
csv_file = "win_records.csv"
if not os.path.exists(csv_file) or os.stat(csv_file).st_size == 0:
    pd.DataFrame(columns=["ì´ë¦„", "ì‹œê°„", "ìŠ¹ë¦¬ íšŸìˆ˜", "ëª¬ìŠ¤í„° MP"]).to_csv(csv_file, index=False)

# âœ… ì„¸ì…˜ ë³€ìˆ˜ ì´ˆê¸°í™”
if "monster_mp" not in st.session_state:
    st.session_state.monster_mp = 50  # ê¸°ë³¸ê°’ ì„¤ì •
if "initial_mp" not in st.session_state:
    st.session_state.initial_mp = st.session_state.monster_mp

# âœ… Streamlit UI
st.subheader("ğŸ® ê²Œì„ì„ ì‹œì‘í•©ë‹ˆë‹¤!")
st.info('ğŸ“¸ ì´ˆë¡ìƒ‰ ë„¤ëª¨ ë°•ìŠ¤ ì•ˆì— ì†ì„ ìœ„ì¹˜ì‹œì¼œì£¼ì„¸ìš”.')

# âœ… WebRTCë¥¼ ì‚¬ìš©í•œ ì‹¤ì‹œê°„ ì›¹ìº 
class VideoTransformer(VideoTransformerBase):
    def __init__(self):
        self.last_captured_time = time.time()
        self.capture_interval = 3  # 3ì´ˆë§ˆë‹¤ ìº¡ì²˜
        self.frame_to_analyze = None

    def transform(self, frame):
        img = frame.to_ndarray(format="bgr24")
        h, w, _ = img.shape

        # ì´ˆë¡ ë„¤ëª¨ ë°•ìŠ¤ ì„¤ì •
        box_size = min(h, w) // 2
        x1, y1 = (w - box_size) // 2, (h - box_size) // 2
        x2, y2 = x1 + box_size, y1 + box_size
        cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)  # ì´ˆë¡ ë„¤ëª¨

        # 3ì´ˆë§ˆë‹¤ ì´ë¯¸ì§€ ìº¡ì²˜
        if time.time() - self.last_captured_time > self.capture_interval:
            self.frame_to_analyze = img[y1:y2, x1:x2]  # ë°•ìŠ¤ ì•ˆì˜ ë¶€ë¶„ë§Œ ì €ì¥
            self.last_captured_time = time.time()

        return av.VideoFrame.from_ndarray(img, format="bgr24")

webrtc_ctx = webrtc_streamer(
    key="game",
    video_transformer_factory=VideoTransformer,
    async_transform=True
)

# âœ… 3ì´ˆë§ˆë‹¤ ìº¡ì²˜ëœ í”„ë ˆì„ì„ ë¶„ì„
if webrtc_ctx.video_transformer and webrtc_ctx.video_transformer.frame_to_analyze is not None:
    captured_img = webrtc_ctx.video_transformer.frame_to_analyze
    img = cv2.cvtColor(captured_img, cv2.COLOR_BGR2RGB)
    img = Image.fromarray(img).resize((224, 224))
    img_array = np.array(img, dtype=np.float32) / 255.0
    img_array = np.expand_dims(img_array, axis=0)

    # ğŸ¤– ëª¨ë¸ ì˜ˆì¸¡
    prediction = model.predict(img_array)
    class_index = np.argmax(prediction)
    confidence = np.max(prediction)

    if confidence >= 0.7:
        user_choice = class_names[class_index]
        monster_choice = np.random.choice(["ê°€ìœ„", "ë°”ìœ„", "ë³´"])

        game_result = "âœ… ìŠ¹ë¦¬" if (user_choice, monster_choice) in [("ê°€ìœ„", "ë³´"), ("ë°”ìœ„", "ê°€ìœ„"), ("ë³´", "ë°”ìœ„")] else "âŒ íŒ¨ë°°"
        st.subheader(f"ğŸ– ë‚´ ì„ íƒ: {user_choice}  VS  ğŸ‘¾ ëª¬ìŠ¤í„° ì„ íƒ: {monster_choice}")
        st.markdown(f"### ê²°ê³¼ â¡ï¸ **{game_result}**")

